{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "KbxpIeXTdw2n",
   "metadata": {
    "id": "KbxpIeXTdw2n"
   },
   "source": [
    "# Self-study try-it activity 1.1: Optimisation essentials \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x56fGpt5d3TG",
   "metadata": {
    "id": "x56fGpt5d3TG"
   },
   "source": [
    "This notebook explores essential concepts in machine learning:\n",
    "\n",
    "1. **Gradient descent algorithm**: gradient descent is an optimisation algorithm that iteratively adjusts model parameters to minimise a loss function by moving in the direction of the steepest descent.\n",
    "   \n",
    "2. **Loss functions**: a loss function (also called a cost function or an objective function) measures the difference between quantifies the error or cost between the predicted values of a model and the actual values.\n",
    "\n",
    "3. **Activation functions**: activation functions are mathematical tools applied to neural network outputs to introduce non-linearity, enabling networks to learn complex patterns.\n",
    "\n",
    "4. **Regularisation techniques**: regularisation in machine learning is a technique to prevent overfitting by adding a penalty to the model's complexity, ensuring it learns general patterns rather than memorising noise, thereby improving its performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d967ee-9077-41ee-87ea-fe554f542e6a",
   "metadata": {},
   "source": [
    "## Gradient descent algorithm\n",
    "\n",
    "This activity discusses the gradient descent algorithm, which is given by: \n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\alpha\\nabla J(\\theta)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ccf564-d9d9-4d0f-b76f-9a901583a21c",
   "metadata": {},
   "source": [
    "Gradient descent updates the parameter $\\theta$ by subtracting the derivative of the cost function $J(\\theta)$ with respect to $\\theta$, guiding the model towards minimising $J$.\n",
    "\n",
    "$\\alpha$ controls the step size; too large can cause divergence, while too small slows convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523b42d-8a35-41f0-b86a-a60da9a2f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa39bb66-036f-4e6b-ab54-4f85729564ef",
   "metadata": {},
   "source": [
    "The gradient of the function is $f'(x) = 2(x - 3)$. We update the value of $x$ by moving in the direction opposite to the gradient.\n",
    "\n",
    "In this case, the learning rate ($\\alpha$) is 0.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6480dc35-ca8d-450e-bcc5-215558d6c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given function: f(x) = (x - 3)^2\n",
    "# Derivative: f'(x) = 2(x - 3)\n",
    "# Initial value: x0 = 10\n",
    "# Learning rate: alpha = 0.1\n",
    "# Perform three iterations\n",
    "\n",
    "x = 10  # initial guess\n",
    "alpha = 0.1  # learning rate\n",
    "iterations = 3  # number of iterations\n",
    "# Perform gradient descent updates\n",
    "history = [x]\n",
    "for _ in range(iterations):\n",
    "    gradient = 2 * (x - 3)  # Compute derivative f'(x)\n",
    "    x = x - alpha * gradient  # Update x\n",
    "    history.append(x)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3309f6-1a63-461d-a7b2-1b375c660e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it yourself\n",
    "# Vary the number of iterations to observe how the value of x converges to the minimum\n",
    "# Modify the learning rate to see how it affects the convergence rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j91bKuBRi7ay",
   "metadata": {
    "id": "j91bKuBRi7ay"
   },
   "source": [
    "## Loss functions\n",
    "\n",
    "This activity discusses two types of loss functions.\n",
    "\n",
    "1. **Mean squared error (MSE)**: MSE calculates the average of the squared differences between predicted and actual values.\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "2. **Cross-entropy loss (log loss)**: this measures the difference between the probability distributions of the predicted and actual categories.\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log (\\hat{y}_i) + (1 - y_i) \\log (1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb5ab9-1f66-42ec-98e1-180a3fad7ae6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "06cb5ab9-1f66-42ec-98e1-180a3fad7ae6",
    "outputId": "b70efc98-60af-455b-8b07-da33788e1ff9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to compute the MSE\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the MSE between true and predicted values.\n",
    "    Parameters:\n",
    "    y_true (array-like): Actual values\n",
    "    y_pred (array-like): Predicted values\n",
    "    Returns:\n",
    "    float: mean squared error\n",
    "    \"\"\"\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Regression example (MSE calculation)\n",
    "y_true = np.array([3, 5, 6, 8])  # Actual values\n",
    "y_pred = np.array([2.5, 4.8, 6.1, 7.9])  # Predicted values\n",
    "\n",
    "\n",
    "# Compute the MSE\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# Print the MSE\n",
    "print(\"The mean squared error is given by \", mse)\n",
    "\n",
    "# Plot the MSE\n",
    "# Plot actual vs predicted\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_true, y_pred, color='blue', label='Predictions')\n",
    "plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], color='red', linestyle='--', label='Perfect Predictions')\n",
    "\n",
    "# Annotate the errors\n",
    "for i in range(len(y_true)):\n",
    "    plt.plot([y_true[i], y_true[i]], [y_true[i], y_pred[i]], 'k--')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(f\"Actual vs Predicted (MSE = {mse:.2f})\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HqU8xHy5ss-A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "HqU8xHy5ss-A",
    "outputId": "70bd3515-c3b1-4288-c3ef-9aa7b2b59dfd"
   },
   "outputs": [],
   "source": [
    "# Function to compute the cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss for binary classification.\n",
    "    Parameters:\n",
    "    y_true (array-like): Actual class labels (0 or 1)\n",
    "    y_pred (array-like): Predicted probabilities (between 0 and 1)\n",
    "    Returns:\n",
    "    float: cross-entropy loss\n",
    "    \"\"\"\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Classification example (cross-entropy loss calculation)\n",
    "y_true_classification = np.array([1, 0, 1, 1])  # Actual labels (0 or 1)\n",
    "y_pred_classification = np.array([0.9, 0.1, 0.8, 0.7])  # Predicted probabilities\n",
    "\n",
    "# Compute the cross-entropy loss\n",
    "cross_entropy = cross_entropy_loss(y_true_classification, y_pred_classification)\n",
    "\n",
    "# Print the cross-entropy\n",
    "print(\"The cross entropy loss is given by \", cross_entropy)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Scatter plot of predicted probabilities vs. loss\n",
    "individual_losses = -(y_true_classification * np.log(y_pred_classification) + (1 - y_true_classification) * np.log(1 - y_pred_classification))\n",
    "plt.scatter(y_pred_classification, individual_losses, color='blue', label='Loss per prediction', zorder=2)\n",
    "\n",
    "# Line plot showing general loss trend\n",
    "x_vals = np.linspace(0.01, 0.99, 100)  # Avoid log(0) error\n",
    "y_vals = - (1 * np.log(x_vals) + (0 * np.log(1 - x_vals)))  # Loss for positive class (y_true=1)\n",
    "plt.plot(x_vals, y_vals, color='red', linestyle='--', label='Cross-Entropy Loss Curve')\n",
    "\n",
    "# Bar plot for individual loss values\n",
    "plt.bar(range(len(y_true)), cross_entropy, color='orange', alpha=0.6, label='Individual Loss')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(f\"Cross-Entropy Loss Visualization (Total Loss = {cross_entropy_loss(y_true, y_pred):.4f})\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7GWLGOirMZD",
   "metadata": {
    "id": "d7GWLGOirMZD"
   },
   "outputs": [],
   "source": [
    "# Compute the MSE for these values\n",
    "# Try it yourself\n",
    "y_true = np.array([2,4,6,8,10])  # Actual values\n",
    "y_pred = np.array([2.5, 4.8, 6.1, 7.9, 10.2])  # Predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GG4hrXlivUQE",
   "metadata": {
    "id": "GG4hrXlivUQE"
   },
   "outputs": [],
   "source": [
    "# Compute the cross-entropy for these values\n",
    "# Try it yourself\n",
    "y_true_classification = np.array([1, 0, 1, 1, 0])  # Actual labels (0 or 1)\n",
    "y_pred_classification = np.array([0.9, 0.1, 0.8, 0.7, 0.2])  # Predicted probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urZHHLrtwu0t",
   "metadata": {
    "id": "urZHHLrtwu0t"
   },
   "source": [
    "## Activation function\n",
    "This activity discusses two commonly used activation functions.\n",
    "\n",
    "1. Sigmoidal\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "\n",
    "2. ReLU\n",
    "\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aaaa6b-eef3-48be-8eb0-7affdf2e2015",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05aaaa6b-eef3-48be-8eb0-7affdf2e2015",
    "outputId": "b32b0a66-072e-4df7-b221-878b654ae44e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Given the inputs\n",
    "x_values = np.array([-2, -1, 0, 1, 2])\n",
    "\n",
    "# Calculate the outputs\n",
    "sigmoid_outputs = sigmoid(x_values)\n",
    "relu_outputs = relu(x_values)\n",
    "\n",
    "# Display the results\n",
    "sigmoid_outputs, relu_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_PtC19aB-Osc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "_PtC19aB-Osc",
    "outputId": "cf5ff767-009d-449c-8c74-89a60e7f7a31"
   },
   "outputs": [],
   "source": [
    "# Generate a range of input values for plotting\n",
    "x_values = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Calculate the outputs\n",
    "sigmoid_outputs = sigmoid(x_values)\n",
    "relu_outputs = relu(x_values)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(2, figsize=(8, 6))\n",
    "\n",
    "# Plot the sigmoid function\n",
    "axs[0].plot(x_values, sigmoid_outputs)\n",
    "axs[0].set_title('Sigmoid Activation Function')\n",
    "axs[0].set_xlabel('Input')\n",
    "axs[0].set_ylabel('Output')\n",
    "\n",
    "# Plot the ReLU function\n",
    "axs[1].plot(x_values, relu_outputs)\n",
    "axs[1].set_title('ReLU Activation Function')\n",
    "axs[1].set_xlabel('Input')\n",
    "axs[1].set_ylabel('Output')\n",
    "\n",
    "# Layout so the plots do not overlap\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-vGgVJn8_vfW",
   "metadata": {
    "id": "-vGgVJn8_vfW"
   },
   "outputs": [],
   "source": [
    "# Try it yourself\n",
    "array = np.arange(-10, 11)\n",
    "# For this array, create sigmoid and ReLU activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UlilDdV9swha",
   "metadata": {
    "id": "UlilDdV9swha"
   },
   "source": [
    "## Regularisation\n",
    "\n",
    "Regularisation is a technique used to prevent overfitting in machine learning models. The main types of regularisation methods are:\n",
    "\n",
    "- Lasso regression, or L1 regularisation\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} L(y_i, \\hat{y}_i) + \\lambda \\sum_{j=1}^{n} |\\theta_j|\n",
    "$$\n",
    "\n",
    "- Ridge regression, or L2 regularisation\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} L(y_i, \\hat{y}_i) + \\lambda \\sum_{j=1}^{n} (\\theta_j)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eefbca6-8600-4d39-8532-b2dbad1752ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "6eefbca6-8600-4d39-8532-b2dbad1752ce",
    "outputId": "186577ef-dff4-4bec-afc1-2b6dd943604b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 5 * X + np.random.randn(100, 1)  # y = 4 + 5x + noise\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train different regression models\n",
    "lin_reg = LinearRegression()\n",
    "ridge_reg = Ridge(alpha=0.2)  # L2 regularization\n",
    "lasso_reg = Lasso(alpha=0.2)  # L1 regularization\n",
    "\n",
    "lin_reg.fit(X_train, y_train)\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions for plotting\n",
    "X_plot = np.linspace(0, 2, 100).reshape(-1, 1)\n",
    "y_pred_lin = lin_reg.predict(X_plot)\n",
    "y_pred_ridge = ridge_reg.predict(X_plot)\n",
    "y_pred_lasso = lasso_reg.predict(X_plot)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train, y_train, color='gray', alpha=0.5, label=\"Training Data\")\n",
    "plt.scatter(X_test, y_test, color='red', alpha=0.7, label=\"Test Data\")\n",
    "plt.plot(X_plot, y_pred_lin, \"b-\", label=\"Linear Regression\")\n",
    "plt.plot(X_plot, y_pred_ridge, \"g--\", label=\"Ridge Regression (L2)\")\n",
    "plt.plot(X_plot, y_pred_lasso, \"r-.\", label=\"Lasso Regression (L1)\")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title(\"Linear vs Ridge (L2) vs Lasso (L1) Regression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c5c4e9-3a98-4eba-aaac-bf0b3ca25c82",
   "metadata": {
    "id": "81c5c4e9-3a98-4eba-aaac-bf0b3ca25c82"
   },
   "outputs": [],
   "source": [
    "# For this generated data, create the lasso regression and ridge regression\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(1000, 1)\n",
    "y = 3 + 2 * X + np.random.randn(1000, 1)  # y = 4 + 5x + noise\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
